{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from ultralytics import YOLO, settings\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import requests\n",
    "import shutil\n",
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "CUDA = torch.cuda.is_available()\n",
    "print(\"CUDA is available:\", CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_SEGMENT = '../models/plant_segmentation_v1.pt'\n",
    "IMG_PATH_FIXED = '../images/cropped_scales/fixed'\n",
    "IMG_PATH_RANDOM = '../images/cropped_scales/random'\n",
    "DATA_PATH = '../data/processed'\n",
    "DEVICE = \"cuda\" if CUDA else \"cpu\"\n",
    "settings.update({'runs_dir': rf'/home/floris/Projects/NTNU/models/runs'})\n",
    "\n",
    "model_seg = YOLO(MODEL_PATH_SEGMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_scale_fixed_via_colorboard(image_path):\n",
    "    \"\"\"\n",
    "    Function to measure the scale of the image using a fixed colorboard\n",
    "    The colorboard is a set of 7 boxes with bright colors\n",
    "    The width of the colorboard is 4.5 cm\n",
    "    The function returns the pixels per cm\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert to HSV (Hue, Saturation, Value) color space for easier color segmentation\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define range of bright colors in HSV\n",
    "    lower_color = np.array([0, 100, 100])\n",
    "    upper_color = np.array([179, 255, 255])\n",
    "\n",
    "    # Threshold the HSV image to get only bright colors\n",
    "    mask = cv2.inRange(hsv, lower_color, upper_color)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter out small contours that are not our boxes\n",
    "    box_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 100]\n",
    "\n",
    "    # Calculate bounding boxes for each contour\n",
    "    bounding_boxes = [cv2.boundingRect(cnt) for cnt in box_contours]\n",
    "\n",
    "    # Determine the midpoint of the image width\n",
    "    midpoint = image.shape[1] / 2\n",
    "\n",
    "    # Keep only the boxes that have an x-coordinate greater than the midpoint\n",
    "    right_half_boxes = [box for box in bounding_boxes if box[0] > midpoint]\n",
    "\n",
    "    # Sort these boxes by their x-coordinate to ensure rightmost first\n",
    "    sorted_right_half_boxes = sorted(right_half_boxes, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Draw these seven boxes on the image\n",
    "    for (x, y, w, h) in sorted_right_half_boxes:\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    # Calculate the total width in pixels of these seven boxes\n",
    "    total_width_in_pixels = sum([box[2] for box in sorted_right_half_boxes])\n",
    "\n",
    "    # Since 7 boxes = 4.5 cm, calculate the pixels per cm\n",
    "    pixels_per_cm = total_width_in_pixels / 5.79\n",
    "\n",
    "    return pixels_per_cm\n",
    "\n",
    "def transform_px_to_cm(box, px_per_cm):\n",
    "    \"\"\"\n",
    "    Function to transform the width and height of a box from pixels to cm\n",
    "    \"\"\"\n",
    "    w = np.abs((box[2] - box[0]).cpu())\n",
    "    h = np.abs((box[3] - box[1]).cpu())\n",
    "    return w / px_per_cm, h / px_per_cm\n",
    "\n",
    "def get_masked_image(image, mask):\n",
    "    \"\"\"\n",
    "    Apply a mask to an image with transparency\n",
    "    \"\"\"\n",
    "    # Remove single-dimensional entry from the shape of the mask\n",
    "    mask_squeezed = np.squeeze(mask)  # This should change mask shape to (5831, 3391)\n",
    "    # Generate an alpha channel where mask is True (255) and False (0)\n",
    "    alpha_channel = np.where(mask_squeezed, 255, 0).astype(np.uint8)\n",
    "    # Ensure alpha channel is correctly shaped [H, W] -> [H, W, 1]\n",
    "    alpha_channel_shaped = np.expand_dims(alpha_channel, axis=-1)\n",
    "\n",
    "    # print(\"Image shape:\", image.size)\n",
    "    # print(\"Alpha channel shape:\", alpha_channel_shaped.shape)\n",
    "\n",
    "    # Concatenate the alpha channel with the image to create an RGBA image\n",
    "    rgba_image = np.concatenate((image, alpha_channel_shaped), axis=-1)\n",
    "    return rgba_image\n",
    "\n",
    "def get_cropped_image(image, box):\n",
    "    \"\"\"\n",
    "    Crop an image with a given box\n",
    "    \"\"\"\n",
    "    if isinstance(box, list):\n",
    "        box = box[0]\n",
    "    if isinstance(box, torch.Tensor):\n",
    "        box = box.cpu().numpy() \n",
    "    else:\n",
    "        box = np.array(box) \n",
    "\n",
    "    if len(box.shape) > 1:\n",
    "        box = box[0]\n",
    "    x, y, w, h = int(box[0]), int(box[1]), int(box[2] - box[0]), int(box[3] - box[1])\n",
    "    return image[y:y+h, x:x+w]\n",
    "\n",
    "def apply_crop_mask(image, mask, box):\n",
    "    \"\"\"\n",
    "    Apply a mask to an image and crop the image with a given box\n",
    "    Returns a list of tuples with the masked image and the cropped image\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    if len(np.array(mask).shape) == 3:\n",
    "        for i, m in enumerate(mask):\n",
    "            m_img = get_masked_image(image, m)\n",
    "            b = box[i].cpu().numpy() if type(box) == torch.Tensor else box[i]\n",
    "            crop_img = get_cropped_image(m_img, b)\n",
    "            images.append((m_img, crop_img))\n",
    "    else:\n",
    "        m_img = get_masked_image(image, mask)\n",
    "        b = box.cpu().numpy() if type(box) == torch.Tensor else box\n",
    "        crop_img = get_cropped_image(m_img, b)\n",
    "        images.append((m_img, crop_img))\n",
    "    return images\n",
    "\n",
    "def find_dominant_color(image, k=3):\n",
    "    # Convert image to numpy array\n",
    "    img_array = np.array(image)\n",
    "    # Reshape it to a list of RGB values\n",
    "    img_vector = img_array.reshape((-1, 3))\n",
    "    # Run k-means on the pixel colors (fit only on a subsample to speed up)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(img_vector[::50])\n",
    "    # Get the dominant color\n",
    "    dominant_color = kmeans.cluster_centers_[np.argmax(np.bincount(kmeans.labels_))]\n",
    "    # Create a mask for pixels within a certain distance from the dominant color\n",
    "    distances = np.sqrt(np.sum((img_vector - dominant_color) ** 2, axis=1))\n",
    "    mask = distances < np.std(distances)\n",
    "    # Turn the dominant color range to white\n",
    "    img_vector[mask] = [255, 255, 255]\n",
    "    result_img_array = img_vector.reshape(img_array.shape)\n",
    "    # turn image back to PIL\n",
    "    result_img = Image.fromarray(result_img_array.astype(np.uint8))\n",
    "    return dominant_color, result_img\n",
    "\n",
    "def calculate_mask_area(masked_pixels, pixels_per_cm):\n",
    "    area_square_cm = masked_pixels / (pixels_per_cm ** 2)\n",
    "    return area_square_cm\n",
    "\n",
    "def get_images(path, range_left=0, range_right=-1):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Path {path} does not exist\")\n",
    "        return []\n",
    "    if len(os.listdir(path)) == 0:\n",
    "        print(f\"Path {path} is empty\")\n",
    "        return []\n",
    "    \n",
    "    images = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.jpg') and 'only' not in f and 'grid' not in f]\n",
    "    return images[range_left:range_right]\n",
    "\n",
    "def calc_non_transparent_pixel_count(image):\n",
    "    image = Image.open(image)\n",
    "    img_arr = np.array(image)\n",
    "    ntp = np.where(img_arr[:, :, 3] != 0)\n",
    "    return len(ntp[0])\n",
    "\n",
    "def generate_output(images, model):\n",
    "\n",
    "    results = model(images, verbose=False, retina_masks=True, conf=0.5)\n",
    "    output = []\n",
    "\n",
    "    for result in results:\n",
    "\n",
    "        image_path = result.path\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        boxes = result.boxes.xyxy\n",
    "        masks = result.masks.data[0].cpu().numpy()\n",
    "\n",
    "        output.append({'image_path': image_path, 'image': image, 'boxes': boxes, 'masks': masks})\n",
    "    \n",
    "    return output\n",
    "\n",
    "def main(output):\n",
    "    \"\"\"\n",
    "    Function to display the output of the model\n",
    "    It displays the image with the boxes and masks, and the width and height of the boxes in cm\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for _, res in enumerate(output):\n",
    "\n",
    "        path = res['image_path']\n",
    "        image = res['image']\n",
    "        mask = res['masks']\n",
    "        boxes = res['boxes']\n",
    "        name = os.path.basename(path)\n",
    "\n",
    "        image = find_dominant_color(image)[1]\n",
    "                \n",
    "        # look for cropped scale\n",
    "        scale_path = path.replace('.jpg', '_scale_only.jpg')\n",
    "        px_per_cm = measure_scale_fixed_via_colorboard(scale_path)\n",
    "\n",
    "        all_masks_with_sq_cm = []\n",
    "\n",
    "        if len(mask.shape) == 3:\n",
    "            for m in mask:\n",
    "                m_sum = m[0].sum().tolist()\n",
    "                square_cm = calculate_mask_area(m_sum, px_per_cm)\n",
    "                all_masks_with_sq_cm.append((m, square_cm))\n",
    "        else:\n",
    "            m_sum = mask.sum().tolist()\n",
    "            square_cm = calculate_mask_area(m_sum, px_per_cm)\n",
    "            all_masks_with_sq_cm.append((mask, square_cm))\n",
    "            \n",
    "\n",
    "        all_boxes = []\n",
    "        for box in boxes:\n",
    "            w_cm, h_cm = transform_px_to_cm(box, px_per_cm)\n",
    "            all_boxes.append((box, {'width_cm': w_cm, 'height_cm': h_cm}))\n",
    " \n",
    "        \n",
    "        masked_and_cropped_images = apply_crop_mask(image, mask, boxes) # list of tuples (masked_image, cropped_image) per box / mask\n",
    "\n",
    "        results.append({'image': image, 'image_path': path, 'image_name': name, 'boxes': all_boxes, 'masks_and_sqcm': all_masks_with_sq_cm, 'px_per_cm': px_per_cm, 'masked_and_cropped_images': masked_and_cropped_images})\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_results(results, path):\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    for res in results:\n",
    "\n",
    "        image = res['image']\n",
    "        mask = [r[0] for r in res['masks_and_sqcm']]\n",
    "        box = [box[0] for box in res['boxes']]\n",
    "\n",
    "        imgs = apply_crop_mask(image, mask, box)\n",
    "        for idx, i in enumerate(imgs):\n",
    "            img_name = res['image_name'].replace('.jpg', f'_plant_mask_crop_{chr(idx + 97)}.png')\n",
    "            img_path = os.path.join(path, img_name)\n",
    "\n",
    "            # to turn the white pixels to transparent\n",
    "            threshold = 250\n",
    "            pil_img = Image.fromarray(i[1])\n",
    "            datas = pil_img.getdata()\n",
    "            new_image_data = []\n",
    "            for item in datas:\n",
    "                if item[0] > threshold and item[1] > threshold and item[2] > threshold:\n",
    "                    new_image_data.append((255, 255, 255, 0))\n",
    "                else:\n",
    "                    new_image_data.append(item)\n",
    "\n",
    "            pil_img.putdata(new_image_data)\n",
    "            pil_img.save(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_images(IMG_PATH_FIXED)\n",
    "\n",
    "output = generate_output(images[0:20], model_seg)\n",
    "\n",
    "results = main(output)\n",
    "\n",
    "save_results(results, DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 31.71 cm²\n",
      "After removing white space: 19.50 cm²\n"
     ]
    }
   ],
   "source": [
    "number = 4\n",
    "sq_cm_bef = results[number]['masks_and_sqcm'][0][1]\n",
    "print(f'Before: {sq_cm_bef:.2f} cm²')\n",
    "img_name = os.path.basename(results[number]['image_path']).replace('.jpg', '_plant_mask_crop_a.png')\n",
    "img_path = os.path.join(DATA_PATH, img_name)\n",
    "ntpx = calc_non_transparent_pixel_count(img_path)\n",
    "area = calculate_mask_area(ntpx, results[number]['px_per_cm'])\n",
    "print(f'After removing white space: {area:.2f} cm²')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
