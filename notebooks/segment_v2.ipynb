{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from ultralytics import YOLO, settings\n",
    "from packages import check_directory\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import shutil\n",
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "CUDA = torch.cuda.is_available()\n",
    "print(\"CUDA is available:\", CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_SEGMENT = '../models/plant_segmentation_v2.pt'\n",
    "IMG_PATH_FIXED = '../images/cropped_scales/fixed'\n",
    "IMG_PATH_RANDOM = '../images/cropped_scales/random'\n",
    "DATA_PATH = '../data/processed'\n",
    "DEVICE = \"cuda\" if CUDA else \"cpu\"\n",
    "settings.update({'runs_dir': rf'/home/floris/Projects/NTNU/models/runs'})\n",
    "\n",
    "model_seg = YOLO(MODEL_PATH_SEGMENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_scale_random(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    _max = 0\n",
    "    _max_idx = 0\n",
    "    for idx, c in enumerate(contours):\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        if h > w:\n",
    "            if abs(w - h) < 10 and w >= 0.18 * image.shape[0]:\n",
    "                _max_idx = idx if w > _max else _max_idx\n",
    "                _max = max(w, _max)\n",
    "        else:\n",
    "            if abs(w - h) < 10 and w >= 0.18 * image.shape[1]:\n",
    "                _max_idx = idx if h > _max else _max_idx\n",
    "                _max = max(h, _max)\n",
    "\n",
    "    _, _, w, h = cv2.boundingRect(contours[_max_idx])\n",
    "\n",
    "    px_per_cm_w = w / 2\n",
    "    px_per_cm_h = h / 2\n",
    "    px_per_cm = (px_per_cm_w + px_per_cm_h) / 2\n",
    "\n",
    "    return px_per_cm\n",
    "\n",
    "def measure_scale_fixed_via_colorboard(image_path, box_width_mm=6.2):\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    lower_red = np.array([100, 0, 0])\n",
    "    upper_red = np.array([255, 100, 100])\n",
    "    mask = cv2.inRange(image_rgb, lower_red, upper_red)\n",
    "\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    largest_contour = max(contours, key=cv2.contourArea)\n",
    "    _, _, w, _ = cv2.boundingRect(largest_contour)\n",
    "    pixels_per_cm = w / (box_width_mm / 10)\n",
    "    return pixels_per_cm\n",
    "\n",
    "def transform_px_to_cm(box, px_per_cm):\n",
    "    \"\"\"\n",
    "    Function to transform the width and height of a box from pixels to cm\n",
    "    \"\"\"\n",
    "    try:\n",
    "        w = np.abs((box[2] - box[0]).cpu())\n",
    "        h = np.abs((box[3] - box[1]).cpu())\n",
    "    except:\n",
    "        w = np.abs(box[2] - box[0])\n",
    "        h = np.abs(box[3] - box[1])\n",
    "    return w / px_per_cm, h / px_per_cm\n",
    "\n",
    "def get_masked_image(image, mask):\n",
    "    \"\"\"\n",
    "    Apply a mask to an image with transparency\n",
    "    \"\"\"\n",
    "    # Remove single-dimensional entry from the shape of the mask\n",
    "    mask_squeezed = np.squeeze(mask)  # This should change mask shape to (5831, 3391)\n",
    "    # Generate an alpha channel where mask is True (255) and False (0)\n",
    "    alpha_channel = np.where(mask_squeezed, 255, 0).astype(np.uint8)\n",
    "    # Ensure alpha channel is correctly shaped [H, W] -> [H, W, 1]\n",
    "    alpha_channel_shaped = np.expand_dims(alpha_channel, axis=-1)\n",
    "\n",
    "    # print(\"Image shape:\", image.size)\n",
    "    # print(\"Alpha channel shape:\", alpha_channel_shaped.shape)\n",
    "\n",
    "    # Concatenate the alpha channel with the image to create an RGBA image\n",
    "    rgba_image = np.concatenate((image, alpha_channel_shaped), axis=-1)\n",
    "    return rgba_image\n",
    "\n",
    "def get_cropped_image(image, box):\n",
    "    \"\"\"\n",
    "    Crop an image with a given box\n",
    "    \"\"\"\n",
    "    if isinstance(box, list):\n",
    "        box = box[0]\n",
    "    if isinstance(box, torch.Tensor):\n",
    "        box = box.cpu().numpy() \n",
    "    else:\n",
    "        box = np.array(box) \n",
    "\n",
    "    if len(box.shape) > 1:\n",
    "        box = box[0]\n",
    "    x, y, w, h = int(box[0]), int(box[1]), int(box[2] - box[0]), int(box[3] - box[1])\n",
    "    return image[y:y+h, x:x+w]\n",
    "\n",
    "def apply_crop_mask(image, mask, box):\n",
    "    \"\"\"\n",
    "    Apply a mask to an image and crop the image with a given box\n",
    "    Returns a list of tuples with the masked image and the cropped image\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    if len(np.array(mask).shape) == 3:\n",
    "        for i, m in enumerate(mask):\n",
    "            m_img = get_masked_image(image, m)\n",
    "            b = box[i].cpu().numpy() if type(box) == torch.Tensor else box[i]\n",
    "            crop_img = get_cropped_image(m_img, b)\n",
    "            images.append((m_img, crop_img))\n",
    "    else:\n",
    "        m_img = get_masked_image(image, mask)\n",
    "        b = box.cpu().numpy() if type(box) == torch.Tensor else box\n",
    "        crop_img = get_cropped_image(m_img, b)\n",
    "        images.append((m_img, crop_img))\n",
    "    return images\n",
    "\n",
    "def find_dominant_color(image, k=3):\n",
    "    # Convert image to numpy array\n",
    "    img_array = np.array(image)\n",
    "    # Reshape it to a list of RGB values\n",
    "    img_vector = img_array.reshape((-1, 3))\n",
    "    # Run k-means on the pixel colors (fit only on a subsample to speed up)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(img_vector[::50])\n",
    "    # Get the dominant color\n",
    "    dominant_color = kmeans.cluster_centers_[np.argmax(np.bincount(kmeans.labels_))]\n",
    "    # Create a mask for pixels within a certain distance from the dominant color\n",
    "    distances = np.sqrt(np.sum((img_vector - dominant_color) ** 2, axis=1))\n",
    "    mask = distances < np.std(distances)\n",
    "    # Turn the dominant color range to white\n",
    "    img_vector[mask] = [255, 255, 255]\n",
    "    result_img_array = img_vector.reshape(img_array.shape)\n",
    "    # turn image back to PIL\n",
    "    result_img = Image.fromarray(result_img_array.astype(np.uint8))\n",
    "    return dominant_color, result_img\n",
    "\n",
    "def calculate_mask_area(masked_pixels, pixels_per_cm):\n",
    "    area_square_cm = masked_pixels / (pixels_per_cm ** 2)\n",
    "    return area_square_cm\n",
    "\n",
    "def get_images(path, range_left=0, range_right=-1):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Path {path} does not exist\")\n",
    "        return []\n",
    "    if len(os.listdir(path)) == 0:\n",
    "        print(f\"Path {path} is empty\")\n",
    "        return []\n",
    "    \n",
    "    images = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.jpg') and 'only' not in f and 'grid' not in f]\n",
    "    return images[range_left:range_right]\n",
    "\n",
    "def calc_non_transparent_pixel_count(image):\n",
    "    if type(image) == str:\n",
    "        image = Image.open(image)\n",
    "    if type(image) == np.ndarray:\n",
    "        image = Image.fromarray(image)\n",
    "    if type(image) == Image.Image:\n",
    "        image = image\n",
    "    assert image.mode == 'RGBA', \"Image is not in RGBA mode\"\n",
    "    img_arr = np.array(image)\n",
    "    ntp = np.where(img_arr[:, :, 3] != 0)\n",
    "    return len(ntp[0])\n",
    "\n",
    "def white_to_transparent(image, threshold=250):\n",
    "    pil_img = Image.fromarray(image)\n",
    "    assert pil_img.mode == 'RGBA'\n",
    "    datas = pil_img.getdata()\n",
    "    new_image_data = []\n",
    "    for item in datas:\n",
    "        if item[0] > threshold and item[1] > threshold and item[2] > threshold:\n",
    "            new_image_data.append((255, 255, 255, 0))\n",
    "        else:\n",
    "            new_image_data.append(item)\n",
    "    pil_img.putdata(new_image_data)\n",
    "    return pil_img\n",
    "\n",
    "def get_results(image, path, model, _class, conf, fdc=True):\n",
    "\n",
    "    run_path = '/home/floris/Projects/NTNU/models/runs/segment'\n",
    "    if os.path.exists(run_path):\n",
    "        shutil.rmtree(run_path)\n",
    "\n",
    "    if fdc:\n",
    "        image = find_dominant_color(image)[1]\n",
    "    results = model(image, retina_masks=True, verbose=False, conf=conf)\n",
    "\n",
    "    processed_results = []\n",
    "\n",
    "    for res in results:\n",
    "        boxes = res.boxes.xyxy.cpu().numpy()\n",
    "        masks = res.masks.data.cpu().numpy()\n",
    "        original = res.orig_img\n",
    "        original = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)\n",
    "        masked_cropped_images = apply_crop_mask(original, masks, boxes)\n",
    "        image_path = path\n",
    "\n",
    "        scale_only_path = image_path.replace('.jpg', '_scale_only.jpg')\n",
    "        if _class == 'fixed':\n",
    "            px_per_cm = measure_scale_fixed_via_colorboard(scale_only_path)\n",
    "        if _class == 'random':\n",
    "            px_per_cm = measure_scale_random(scale_only_path)\n",
    "\n",
    "        all_boxes = []\n",
    "        for box in boxes:\n",
    "            w_cm, h_cm = transform_px_to_cm(box, px_per_cm)\n",
    "            all_boxes.append((box, {'width_cm': w_cm, 'height_cm': h_cm}))\n",
    "        \n",
    "        all_mask_crop_with_sq_cm = []\n",
    "        for m_img, c_img in masked_cropped_images:\n",
    "            pil_img = white_to_transparent(c_img)\n",
    "            ntpx = calc_non_transparent_pixel_count(pil_img)\n",
    "            area = calculate_mask_area(ntpx, px_per_cm)\n",
    "            all_mask_crop_with_sq_cm.append((m_img, c_img, area))\n",
    "        \n",
    "        processed_results.append({'image': original, 'image_path': image_path, 'px_per_cm': px_per_cm, 'boxes': all_boxes, 'masks_crops_sqcm': all_mask_crop_with_sq_cm})\n",
    "        \n",
    "    return processed_results\n",
    "\n",
    "def save_results(path, save_path, range_left=0, range_right=-1):\n",
    "    check_directory(save_path)\n",
    "\n",
    "    images = get_images(path)\n",
    "    class_name = path.split('/')[-1]\n",
    "    PIL_images = [Image.open(i) for i in images]\n",
    "\n",
    "    for idx, image in enumerate(PIL_images[range_left:range_right]):\n",
    "        \n",
    "        results = get_results(image, images[idx], model_seg, class_name, 0.7, fdc=True)\n",
    "\n",
    "        res = results[0]\n",
    "\n",
    "        image = Image.fromarray(res[\"image\"])\n",
    "        image_name = os.path.basename(res[\"image_path\"])\n",
    "\n",
    "        image_save_path = os.path.join(save_path, image_name.replace('.jpg', '').replace('_a', '').replace('_b', '').replace('_c', ''))\n",
    "        check_directory(image_save_path)\n",
    "\n",
    "        image.save(os.path.join(image_save_path, image_name))\n",
    "\n",
    "        save_dict = {\"px_per_cm\": res[\"px_per_cm\"], \"boxes\": [], \"sq_cm\": []}\n",
    "\n",
    "        for idx, box in enumerate(res[\"boxes\"]):\n",
    "            box_sep = box[0].tolist()\n",
    "            cm_dict = box[1]\n",
    "            save_dict[\"boxes\"].append({f\"box_{idx}\": box_sep, \"width_cm\": cm_dict[\"width_cm\"], \"height_cm\": cm_dict[\"height_cm\"]})\n",
    "\n",
    "        for idx2, mask_crop in enumerate(res[\"masks_crops_sqcm\"]):\n",
    "            crop = white_to_transparent(mask_crop[1])\n",
    "            crop_name = image_name.replace('.jpg', f'_crop_{idx2}.png')\n",
    "            crop.save(os.path.join(image_save_path, crop_name))\n",
    "\n",
    "            area = mask_crop[2]\n",
    "            save_dict[\"sq_cm\"].append({\"id\": idx2, \"area\": area})\n",
    "        \n",
    "        json_file_name = image_name.replace('.jpg', '.json')\n",
    "        with open(os.path.join(image_save_path, json_file_name), 'w') as f:\n",
    "            json.dump(save_dict, f)\n",
    "\n",
    "def show_img_mask_crop(images, original):\n",
    "    images = [i[1] for i in images]\n",
    "    images.insert(0, original)\n",
    "\n",
    "    n_cols = 3\n",
    "    total_images = len(images)\n",
    "    n_rows = total_images // n_cols + (1 if total_images % n_cols > 0 else 0)\n",
    "\n",
    "    fig, ax = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
    "    ax = ax.flatten()\n",
    "\n",
    "    for idx, img in enumerate(images):\n",
    "        ax[idx].imshow(img)\n",
    "        ax[idx].axis('off')\n",
    "\n",
    "    for idx in range(len(images), len(ax)):\n",
    "        ax[idx].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_results function:\n",
    "\n",
    "1. finds dominant color (background color) on image and replaces it with RGB(255, 255, 255) (full white) -> returns image with background white\n",
    "2. runs the segment model on this white background image -> returns boxes, masks and original image (and other things that can be found in the 'results')\n",
    "3. take the xyxy boxes, the masks and use apply_crop_mask function -> returns list of tupels which contain the masked_image and the mask_cropped_image\n",
    "4. calculate results\n",
    "    - calculate pixels per cm using the cropped scale of that image\n",
    "    - calculate the width and height per bounding box in cm\n",
    "    - calculate the cmÂ² per transparant image (sum of all non-transparant pixels / (pixels_per_cm ^ 2))\n",
    "5. results returned in a dictionary format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/home/floris/Projects/NTNU/images/cropped_scales/random'\n",
    "save_path = '/home/floris/Projects/NTNU/data/processed/random'\n",
    "save_results(input_path, save_path, range_right=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to \u001b[1m/home/floris/Projects/NTNU/models/runs/segment/predict2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "image = '/home/floris/Projects/NTNU/data/processed/random/1701967440/1701967440.jpg'\n",
    "image = Image.open(image)\n",
    "_, image = find_dominant_color(image)\n",
    "\n",
    "model = YOLO('../models/plant_segmentation_v1.pt')\n",
    "results = model(image, retina_masks=True, verbose=False, conf=0.7, save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
