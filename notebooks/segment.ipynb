{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available: True\n"
     ]
    }
   ],
   "source": [
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from ultralytics import YOLO, settings\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import requests\n",
    "import shutil\n",
    "import torch\n",
    "import json\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "CUDA = torch.cuda.is_available()\n",
    "print(\"CUDA is available:\", CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "    \n",
    "# url = \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\"\n",
    "# r = requests.get(url)\n",
    "\n",
    "# with open(\"sam_vit_h_4b8939.pth\", \"wb\") as f:\n",
    "#     f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_scale_fixed_via_colorboard(image_path):\n",
    "    \"\"\"\n",
    "    Function to measure the scale of the image using a fixed colorboard\n",
    "    The colorboard is a set of 7 boxes with bright colors\n",
    "    The width of the colorboard is 4.5 cm\n",
    "    The function returns the pixels per cm\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    # Convert to HSV (Hue, Saturation, Value) color space for easier color segmentation\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Define range of bright colors in HSV\n",
    "    lower_color = np.array([0, 100, 100])\n",
    "    upper_color = np.array([179, 255, 255])\n",
    "\n",
    "    # Threshold the HSV image to get only bright colors\n",
    "    mask = cv2.inRange(hsv, lower_color, upper_color)\n",
    "\n",
    "    # Find contours in the mask\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter out small contours that are not our boxes\n",
    "    box_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > 100]\n",
    "\n",
    "    # Calculate bounding boxes for each contour\n",
    "    bounding_boxes = [cv2.boundingRect(cnt) for cnt in box_contours]\n",
    "\n",
    "    # Determine the midpoint of the image width\n",
    "    midpoint = image.shape[1] / 2\n",
    "\n",
    "    # Keep only the boxes that have an x-coordinate greater than the midpoint\n",
    "    right_half_boxes = [box for box in bounding_boxes if box[0] > midpoint]\n",
    "\n",
    "    # Sort these boxes by their x-coordinate to ensure rightmost first\n",
    "    sorted_right_half_boxes = sorted(right_half_boxes, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "    # Draw these seven boxes on the image\n",
    "    for (x, y, w, h) in sorted_right_half_boxes:\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "    # Calculate the total width in pixels of these seven boxes\n",
    "    total_width_in_pixels = sum([box[2] for box in sorted_right_half_boxes])\n",
    "\n",
    "    # Since 7 boxes = 4.5 cm, calculate the pixels per cm\n",
    "    pixels_per_cm = total_width_in_pixels / 5.79\n",
    "\n",
    "    return pixels_per_cm\n",
    "\n",
    "def generate_output(images, model, predictor):\n",
    "    \"\"\"\n",
    "    Function to generate the output of the model.\n",
    "    Output is a list of dictionaries with the following keys:\n",
    "    - image_path: path to the image\n",
    "    - image: the image\n",
    "    - boxes: the boxes\n",
    "    - masks: the masks\n",
    "    \"\"\"\n",
    "    shutil.rmtree(settings['runs_dir'], ignore_errors=True)\n",
    "    results = model(images, verbose=False)\n",
    "\n",
    "    output = []\n",
    "\n",
    "    for result in results:\n",
    "        image_path = result.path\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        boxes = result.boxes.xyxy\n",
    "\n",
    "        transformed_boxes = predictor.transform.apply_boxes_torch(boxes, image.shape[:2])\n",
    "\n",
    "        predictor.set_image(image)\n",
    "\n",
    "        masks, _, _ = predictor.predict_torch(\n",
    "        point_coords=None,\n",
    "        point_labels=None,\n",
    "        boxes=transformed_boxes,\n",
    "        multimask_output=False,\n",
    "        )\n",
    "\n",
    "        output.append({'image_path': image_path, 'image': image, 'boxes': boxes, 'masks': masks})\n",
    "    \n",
    "    return output\n",
    "\n",
    "def transform_px_to_cm(box, px_per_cm):\n",
    "    \"\"\"\n",
    "    Function to transform the width and height of a box from pixels to cm\n",
    "    \"\"\"\n",
    "    w = np.abs((box[2] - box[0]).cpu())\n",
    "    h = np.abs((box[3] - box[1]).cpu())\n",
    "    return w / px_per_cm, h / px_per_cm\n",
    "\n",
    "def get_masked_image(image, mask):\n",
    "    \"\"\"\n",
    "    Apply a mask to an image with transparency\n",
    "    \"\"\"\n",
    "    # Remove single-dimensional entry from the shape of the mask\n",
    "    mask_squeezed = np.squeeze(mask)  # This should change mask shape to (5831, 3391)\n",
    "    # Generate an alpha channel where mask is True (255) and False (0)\n",
    "    alpha_channel = np.where(mask_squeezed, 255, 0).astype(np.uint8)\n",
    "    # Ensure alpha channel is correctly shaped [H, W] -> [H, W, 1]\n",
    "    alpha_channel_shaped = np.expand_dims(alpha_channel, axis=-1)\n",
    "    # Concatenate the alpha channel with the image to create an RGBA image\n",
    "    rgba_image = np.concatenate((image, alpha_channel_shaped), axis=-1)\n",
    "    return rgba_image\n",
    "\n",
    "def get_cropped_image(image, box):\n",
    "    \"\"\"\n",
    "    Crop an image with a given box\n",
    "    \"\"\"\n",
    "    x, y, w, h = int(box[0]), int(box[1]), int(box[2] - box[0]), int(box[3] - box[1])\n",
    "    return image[y:y+h, x:x+w]\n",
    "\n",
    "def apply_crop_mask(image, mask, box):\n",
    "    \"\"\"\n",
    "    Apply a mask to an image and crop the image with a given box\n",
    "    Returns a list of tuples with the masked image and the cropped image\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    for i, m in enumerate(mask):\n",
    "        m = m.cpu().numpy()\n",
    "        m_img = get_masked_image(image, m)\n",
    "        crop_img = get_cropped_image(m_img, box[i].cpu().numpy())\n",
    "        images.append((m_img, crop_img))\n",
    "    return images\n",
    "\n",
    "def find_dominant_color(image, k=3):\n",
    "    # Convert image to numpy array\n",
    "    img_array = np.array(image)\n",
    "    # Reshape it to a list of RGB values\n",
    "    img_vector = img_array.reshape((-1, 3))\n",
    "    # Run k-means on the pixel colors (fit only on a subsample to speed up)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(img_vector[::50])\n",
    "    # Get the dominant color\n",
    "    dominant_color = kmeans.cluster_centers_[np.argmax(np.bincount(kmeans.labels_))]\n",
    "    # Create a mask for pixels within a certain distance from the dominant color\n",
    "    distances = np.sqrt(np.sum((img_vector - dominant_color) ** 2, axis=1))\n",
    "    mask = distances < np.std(distances)\n",
    "    # Turn the dominant color range to white\n",
    "    img_vector[mask] = [255, 255, 255]\n",
    "    result_img_array = img_vector.reshape(img_array.shape)\n",
    "    # turn image back to PIL\n",
    "    result_img = Image.fromarray(result_img_array.astype(np.uint8))\n",
    "    return dominant_color, result_img\n",
    "\n",
    "def calculate_mask_area(masked_pixels, pixels_per_cm):\n",
    "    area_square_cm = masked_pixels / (pixels_per_cm ** 2)\n",
    "    return area_square_cm\n",
    "\n",
    "def get_images(path, range_left=0, range_right=-1):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Path {path} does not exist\")\n",
    "        return []\n",
    "    if len(os.listdir(path)) == 0:\n",
    "        print(f\"Path {path} is empty\")\n",
    "        return []\n",
    "    \n",
    "    images = [os.path.join(path, f) for f in os.listdir(path) if f.endswith('.jpg') and 'only' not in f and 'grid' not in f]\n",
    "    return images[range_left:range_right]\n",
    "\n",
    "def main(output):\n",
    "    \"\"\"\n",
    "    Function to display the output of the model\n",
    "    It displays the image with the boxes and masks, and the width and height of the boxes in cm\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for _, res in enumerate(output):\n",
    "\n",
    "        path = res['image_path']\n",
    "        image = res['image']\n",
    "        mask = res['masks']\n",
    "        boxes = res['boxes']\n",
    "        name = os.path.basename(path)\n",
    "\n",
    "        image = find_dominant_color(image)[1]\n",
    "                \n",
    "        # look for cropped scale\n",
    "        scale_path = path.replace('.jpg', '_scale_only.jpg')\n",
    "        px_per_cm = measure_scale_fixed_via_colorboard(scale_path)\n",
    "\n",
    "        all_masks_with_sq_cm = []\n",
    "        for m in mask:\n",
    "            m_sum = m[0].sum().tolist()\n",
    "            square_cm = calculate_mask_area(m_sum, px_per_cm)\n",
    "            all_masks_with_sq_cm.append((m, square_cm))\n",
    "            \n",
    "\n",
    "        all_boxes = []\n",
    "        for box in boxes:\n",
    "            w_cm, h_cm = transform_px_to_cm(box, px_per_cm)\n",
    "            all_boxes.append((box, {'width_cm': w_cm, 'height_cm': h_cm}))\n",
    "        \n",
    "        masked_and_cropped_images = apply_crop_mask(image, mask, boxes) # list of tuples (masked_image, cropped_image) per box / mask\n",
    "\n",
    "        results.append({'image': image, 'image_path': path, 'image_name': name, 'boxes': all_boxes, 'masks_and_sqcm': all_masks_with_sq_cm, 'px_per_cm': px_per_cm, 'masked_and_cropped_images': masked_and_cropped_images})\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)\n",
    "    \n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "    \n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2))\n",
    "\n",
    "def show_image(image, ax='Off', figsize=(10, 10)):\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(ax)\n",
    "    plt.show()\n",
    "\n",
    "def show_image_with_box(image, box):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    show_box(box, plt.gca())\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_image_with_boxes_and_masks(image_rgb, boxes, masks):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image_rgb)\n",
    "    for mask in masks:\n",
    "        show_mask(mask.cpu().numpy(), plt.gca())\n",
    "    for box in boxes:\n",
    "        show_box(box.cpu().numpy(), plt.gca())\n",
    "    plt.axis('off')\n",
    "    plt.show() \n",
    "\n",
    "def show_img_mask_crop(image, mask, box):\n",
    "    img = apply_crop_mask(image, mask, box)\n",
    "    num_rows = len(img)\n",
    "    num_cols = max(len(row) for row in img) \n",
    "    \n",
    "    fig, ax = plt.subplots(num_rows, num_cols, figsize=(num_cols*5, num_rows*5))\n",
    "    \n",
    "    for i, row in enumerate(img):\n",
    "        for j, img_ij in enumerate(row):\n",
    "            if num_rows > 1:\n",
    "                ax[i, j].imshow(img_ij)\n",
    "            else: \n",
    "                ax[j].imshow(img_ij)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_PLANT = '../models/plant_detection_v2.pt'\n",
    "MODEL_PATH_TAPE = '../models/tape_detector_v4.pt'\n",
    "IMG_PATH_FIXED = '../images/cropped_scales/fixed'\n",
    "IMG_PATH_RANDOM = '../images/cropped_scales/random'\n",
    "DATA_PATH = '../data/processed'\n",
    "SAM_CHECKPOINT = \"../models/sam_vit_h_4b8939.pth\"\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "DEVICE = \"cuda\" if CUDA else \"cpu\"\n",
    "\n",
    "settings.update({'runs_dir': rf'C:\\Users\\buyse\\Workspace\\NTNU\\models\\runs'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_plant = YOLO(MODEL_PATH_PLANT)\n",
    "model_tape = YOLO(MODEL_PATH_TAPE)\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=SAM_CHECKPOINT)\n",
    "sam.to(device=DEVICE)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = get_images(IMG_PATH_FIXED, 2, 5)\n",
    "output = generate_output(images, model_plant, predictor)\n",
    "results = main(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results:\n",
    "\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        os.makedirs(DATA_PATH, exist_ok=True)\n",
    "\n",
    "    image = res['image']\n",
    "    mask = [r[0] for r in res['masks_and_sqcm']]\n",
    "    box = [box[0] for box in res['boxes']]\n",
    "\n",
    "    imgs = apply_crop_mask(image, mask, box)\n",
    "    for idx, i in enumerate(imgs):\n",
    "        img_name = res['image_name'].replace('.jpg', f'_plant_mask_crop_{chr(idx + 97)}.png')\n",
    "        img_path = os.path.join(DATA_PATH, img_name)\n",
    "\n",
    "        # to turn the white pixels to transparent\n",
    "        threshold = 250\n",
    "        pil_img = Image.fromarray(i[1])\n",
    "        datas = pil_img.getdata()\n",
    "        new_image_data = []\n",
    "        for item in datas:\n",
    "            if item[0] > threshold and item[1] > threshold and item[2] > threshold:\n",
    "                new_image_data.append((255, 255, 255, 0))\n",
    "            else:\n",
    "                new_image_data.append(item)\n",
    "\n",
    "        pil_img.putdata(new_image_data)\n",
    "        pil_img.save(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../data/processed/1698019523_plant_mask_crop_b.png'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "img_array = np.array(img)\n",
    "non_transparent_pixels = np.where(img_array[:, :, 3] != 0)\n",
    "non_transparent_pixels_count = len(non_transparent_pixels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing all white from the picture, the square cm is 87.43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.43493050888728"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mask_area(non_transparent_pixels_count, results[0]['px_per_cm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without removing all white from the picture, the square cm is 92.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.13058401103808"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]['masks_and_sqcm'][1][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
