{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1\n",
      "NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def process_image(image_path, label_path, output_directory, color=(255, 255, 255), threshold=220):\n",
    "\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB\n",
    "    \n",
    "    # find dominant color\n",
    "    dominant_color, image = find_dominant_color(image)\n",
    "    \n",
    "    # save image\n",
    "    image_to_save = Image.fromarray(image)\n",
    "    image_to_save.save(os.path.join(output_directory, 'image_without_dominant_color.png'))\n",
    "\n",
    "    # Load labels\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.strip().split(' ') for line in lines] # removes classnumber and \\n\n",
    "\n",
    "    # Process each polyline\n",
    "    for idx, line in enumerate(lines):\n",
    "        # Skip the class label (at index 0) and process points\n",
    "        points = [[int(float(line[i]) * image.shape[1]), int(float(line[i+1]) * image.shape[0])] for i in range(1, len(line), 2)]\n",
    "        \n",
    "        # Check if we have enough points to form a polyline\n",
    "        if len(points) > 1:\n",
    "            # create mask\n",
    "            mask = np.zeros_like(image)\n",
    "\n",
    "            # Convert points into a numpy array and reshape for polylines\n",
    "            polyline_points = np.array(points, dtype=np.int32).reshape((-1, 1, 2))\n",
    "\n",
    "            # Draw the polyline on the mask with white color and increased thickness\n",
    "            cv2.fillPoly(mask, [polyline_points], color=color)\n",
    "\n",
    "            # Extracting the polyline to a transparent background\n",
    "            extracted = np.zeros((image.shape[0], image.shape[1], 4), dtype=np.uint8)\n",
    "\n",
    "            # Convert mask to grayscale (easier to handle)\n",
    "            mask_gray = cv2.cvtColor(mask, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "            # Copy color from the original image where mask is white\n",
    "            for i in range(3):  # Assuming RGB\n",
    "                extracted[:, :, i] = np.where(mask_gray == 255, image[:, :, i], 0)\n",
    "\n",
    "            # Set the alpha channel: full opacity where mask is white, transparent elsewhere\n",
    "            extracted[:, :, 3] = np.where(mask_gray == 255, 255, 0)\n",
    "\n",
    "            # Call the function to convert white pixels to transparent\n",
    "            final_image = white_to_transparent(extracted, threshold)\n",
    "\n",
    "            # Save the processed image\n",
    "            output_path = os.path.join(output_directory, f'extracted_polyline_{idx}.png')\n",
    "            final_image.save(output_path)\n",
    "        else:\n",
    "            print(f\"Not enough points to form a polyline for line {idx}.\")\n",
    "\n",
    "def white_to_transparent(image, threshold=220):\n",
    "    pil_img = Image.fromarray(image)\n",
    "    assert pil_img.mode == 'RGBA'\n",
    "    datas = pil_img.getdata()\n",
    "    new_image_data = []\n",
    "    for item in datas:\n",
    "        # Checking the RGB channels for whiteness, ignore the alpha channel\n",
    "        if item[0] > threshold and item[1] > threshold and item[2] > threshold:\n",
    "            new_image_data.append((255, 255, 255, 0))  # Full transparency\n",
    "        else:\n",
    "            new_image_data.append(item)\n",
    "    pil_img.putdata(new_image_data)\n",
    "    return pil_img\n",
    "\n",
    "def find_dominant_color(image, k=5):\n",
    "    # Convert image to numpy array\n",
    "    img_array = np.array(image)\n",
    "    # Reshape it to a list of RGB values\n",
    "    img_vector = img_array.reshape((-1, 3))\n",
    "    # Run k-means on the pixel colors (fit only on a subsample to speed up)\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(img_vector)\n",
    "    # Get the dominant color\n",
    "    dominant_color = kmeans.cluster_centers_[np.argmax(np.bincount(kmeans.labels_))]\n",
    "    # Create a mask for pixels within a certain distance from the dominant color\n",
    "    distances = np.sqrt(np.sum((img_vector - dominant_color) ** 2, axis=1))\n",
    "    mask = distances < np.std(distances)\n",
    "    # Turn the dominant color range to white\n",
    "    img_vector[mask] = [255, 255, 255]\n",
    "    result_img_array = img_vector.reshape(img_array.shape)\n",
    "\n",
    "    return dominant_color, result_img_array\n",
    "\n",
    "image_name = '2452362457_jpg.rf.5b826e8d82db1a98cf9422f2dc09ee75'\n",
    "image_path = f'../data/Ntnu_segmentation-24/train/images/{image_name}.jpg'\n",
    "label_path = f'../data/Ntnu_segmentation-24/train/labels/{image_name}.txt'\n",
    "output_path = './test'\n",
    "\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "process_success = process_image(image_path, label_path, output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.34, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Ntnu_segmentation-10 to yolov8:: 100%|██████████| 2731/2731 [00:00<00:00, 2856.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Ntnu_segmentation-10 in yolov8:: 100%|██████████| 87/87 [00:00<00:00, 5856.84it/s]\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.functional import adjust_hue\n",
    "from torchvision import transforms\n",
    "from roboflow import Roboflow\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import torch\n",
    "import os\n",
    "\n",
    "def hue_degree_to_hue_value():\n",
    "    return (torch.rand(1).item() * 60 - 30) / 360.0  # Random hue between -30 and +30 degrees\n",
    "\n",
    "def augment_image(pil_img, name):\n",
    "    images = [pil_img.copy()]  # Start with the original image\n",
    "\n",
    "    # Apply a noticeable hue shift\n",
    "    hue_adjustment = hue_degree_to_hue_value()\n",
    "    images.append(adjust_hue(pil_img.copy(), hue_adjustment))\n",
    "\n",
    "    # Apply a more pronounced saturation change\n",
    "    images.append(transforms.ColorJitter(saturation=0.5)(pil_img.copy()))  # 50% less to 50% more saturation\n",
    "\n",
    "    # Apply a more noticeable brightness change\n",
    "    images.append(transforms.ColorJitter(brightness=0.5)(pil_img.copy()))  # 50% darker to 50% brighter\n",
    "\n",
    "    # Apply a stronger contrast change\n",
    "    images.append(transforms.ColorJitter(contrast=0.5)(pil_img.copy()))  # 50% less to 50% more contrast\n",
    "\n",
    "    names = [name] + [name.replace('.jpg', '_hue.jpg'), name.replace('.jpg', '_sat.jpg'), name.replace('.jpg', '_bright.jpg'), name.replace('.jpg', '_cont.jpg')]\n",
    "    return images, names\n",
    "\n",
    "def augment_images(image_paths):\n",
    "    augmented_batch = []\n",
    "    for img_path in image_paths:\n",
    "        pil_img = Image.open(img_path)  # Open the image file\n",
    "        augmented_batch.append((augment_image(pil_img, os.path.basename(img_path))))  # Apply augmentation and extend the batch list\n",
    "    return augmented_batch\n",
    "\n",
    "# download dataset\n",
    "rf = Roboflow(api_key=\"x5pZ44ydkd9As40Mglzv\")\n",
    "project = rf.workspace(\"ntnuscaledetection\").project(\"ntnu_segmentation\")\n",
    "version = project.version(10)\n",
    "dataset = version.download(\"yolov8\")\n",
    "\n",
    "# get paths\n",
    "path = os.path.join(dataset.location, 'train', 'images')\n",
    "label_path = os.path.join(dataset.location, 'train', 'labels')\n",
    "\n",
    "image_paths = [os.path.join(path, i) for i in os.listdir(path) if i.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# augment images\n",
    "augmented_images = augment_images(image_paths)\n",
    "\n",
    "# save images and labels\n",
    "for idx, item in enumerate(augmented_images):\n",
    "    original_name = item[1][0].replace('.jpg', '.txt')\n",
    "    original_label = os.path.join(label_path, original_name)\n",
    "    # copy original label with new names (hue, sat, bright, cont)\n",
    "    for name in item[1][1:]:\n",
    "        new_label = os.path.join(label_path, name.replace('.jpg', '.txt'))\n",
    "        shutil.copy(original_label, new_label)\n",
    "    \n",
    "    # save images\n",
    "    for name, image in zip(item[1], item[0]):\n",
    "        image.save(os.path.join(path, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deploy model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.34, to fix: `pip install ultralytics==8.0.196`\n",
      "View the status of your deployment at: https://app.roboflow.com/ntnuscaledetection/ntnu_segmentation/5\n",
      "Share your model with the world at: https://universe.roboflow.com/ntnuscaledetection/ntnu_segmentation/model/5\n"
     ]
    }
   ],
   "source": [
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"x5pZ44ydkd9As40Mglzv\")\n",
    "project = rf.workspace(\"ntnuscaledetection\").project(\"ntnu_segmentation\")\n",
    "version = project.version(5)\n",
    "version.deploy(\"yolov8-seg\", \"../models\", \"plant_segmentation_v6.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "images = [os.path.join('../images/gbif_images', image) for image in os.listdir('../images/gbif_images')]\n",
    "\n",
    "\n",
    "for batch in range(0, len(images), 10):\n",
    "    model = YOLO('../models/plant_segmentation_v15.pt')\n",
    "    res = model(images[batch:batch+10], save=True, iou=0.4, conf=0.6, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to \u001b[1m/home/floris/Projects/NTNU/models/runs/segment/predict3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "model = YOLO('../models/plant_segmentation_v15.pt')\n",
    "res = model('../images/gbif_images/1701288394.jpg', save=True, conf=0.2, iou=0.4, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "path = '../models/runs/segment'\n",
    "save_path = '/home/floris/Projects/NTNU/images/segment_eval_v15'\n",
    "\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "preds = os.listdir(path)\n",
    "for dir in preds:\n",
    "    for img in os.listdir(os.path.join(path, dir)):\n",
    "        shutil.move(os.path.join(path, dir, img), save_path)\n",
    "        \n",
    "shutil.rmtree(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def resize_image_with_aspect_ratio(img_path, target_width, target_height):\n",
    "    # Open the original image\n",
    "    img = Image.open(img_path)\n",
    "    \n",
    "    # Calculate the ratio of the target dimensions\n",
    "    target_ratio = target_width / target_height\n",
    "    # Calculate the ratio of the original dimensions\n",
    "    img_ratio = img.width / img.height\n",
    "    \n",
    "    # Determine the size to which the original image is to be resized\n",
    "    if img_ratio > target_ratio:\n",
    "        # Width is the limiting dimension\n",
    "        new_width = target_width\n",
    "        new_height = round(target_width / img_ratio)\n",
    "    else:\n",
    "        # Height is the limiting dimension\n",
    "        new_height = target_height\n",
    "        new_width = round(target_height * img_ratio)\n",
    "    \n",
    "    # Resize the original image\n",
    "    resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Create a new image with a black background and the target size\n",
    "    new_img = Image.new(\"RGB\", (target_width, target_height), (0, 0, 0))\n",
    "    \n",
    "    # Calculate the position to paste the resized image onto the new image\n",
    "    x = (target_width - new_width) // 2\n",
    "    y = (target_height - new_height) // 2\n",
    "    \n",
    "    # Paste the resized image onto the new image\n",
    "    new_img.paste(resized_img, (x, y))\n",
    "    \n",
    "    return new_img\n",
    "\n",
    "\n",
    "image = cv2.imread('../images/upload/1802796785.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('../models/plant_segmentation_v2.pt')\n",
    "results = model(image)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "new_image = resize_image_with_aspect_ratio('../images/upload/1802796785.jpg', 640, 640)\n",
    "\n",
    "plt.imshow(new_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
